---
date: 2025-01-01
layout: post
title: "What it means to be intelligent"
tags: [ai,intelligence,llm]
---



## What Does It Mean to Be Intelligent?

### LLMs: Word Calculators or Something More?

Large Language Models (LLMs) like GPT-4 are often described as "word calculators":

- They don't have an inherent understanding of the world.
- They can't reason in the human sense.
- They can't set their own goals or desires.

Yet, from the seemingly simple objective of next-word prediction, we've seen a host of emergent properties. LLMs can:
- Write code and poetry
- Summarize complex documents
- Pass professional exams
- Hold conversations that feel intelligent

These capabilities make LLMs appear very intelligent, even if their underlying mechanism is just statistical pattern matching.

**Reference:** [Sparks of Artificial General Intelligence: Early experiments with GPT-4 (Microsoft)](https://arxiv.org/abs/2303.12712)

---

### From Philosophy to Experiment: The Chinese Room

The [Chinese Room](https://en.wikipedia.org/wiki/Chinese_room) is a famous thought experiment by philosopher John Searle. Imagine a person who doesn't understand Chinese locked in a room. They receive Chinese characters and use a rulebook (written in English) to produce appropriate Chinese responses. To an outside observer, it seems like the person understands Chinese, but in reality, they're just manipulating symbols.

Today's LLMs are a lot like the Chinese Room: they manipulate symbols (words) based on rules (statistical patterns) without true understanding. Yet, the output can be so convincing that it raises the question: does understanding matter if the results are useful?

**Further reading:**
- [Turing Test](https://en.wikipedia.org/wiki/Turing_test)
- [Searle's Chinese Room Argument](https://plato.stanford.edu/entries/chinese-room/)

---

### Does It Matter If LLMs Aren't "Truly" Intelligent?

Some leading AI researchers, like Yann LeCun, argue that scaling up LLMs won't get us to Artificial General Intelligence (AGI). They believe something architecturally different is needed—LLMs are just "word calculators."

But does it matter, at least for practical purposes?

- If a sufficiently trained model can reason about things (even if it's just next-word prediction) and the reasoning is correct, could we consider that as a form of intelligence?
- Many emergent properties of LLMs can be explained this way. If a system works as per our observations, we treat it as correct—just as in science, where models are updated as new evidence comes in.

**Reference:**
- [Yann LeCun on the limits of scaling LLMs](https://www.technologyreview.com/2023/06/28/1075637/yann-lecun-meta-ai-chief-interview/)

---

### Economic and Societal Impact

Setting aside philosophical debates, the economic implications of AI and LLMs are already profound. The economy doesn't run on "truth"—it runs on what people believe and value (much like the stock market).

LLMs are already:
- Automating customer support
- Assisting in legal and medical research
- Powering new tools for writing, coding, and creativity

The potency of AI and LLMs can't be underestimated, even as we strive for more robust and truly intelligent systems.

**Further reading:**
- [The Economic Case for Generative AI](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)
- [AI and the Future of Work (Brookings)](https://www.brookings.edu/articles/ai-and-the-future-of-work/)

---

### Final Thoughts

Whether or not LLMs are "truly" intelligent, their impact is real and growing. As we continue to observe and experiment, our definitions of intelligence—and our expectations for machines—may evolve as well.
